{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "F4EjBOSxf568"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Install and import the Keras Tuner."
      ],
      "metadata": {
        "id": "1fv4qTIGiqVQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "metadata": {
        "id": "PGMdL5_BjAsu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "VMGRcxLNjDrI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "L-_tE2khjHlJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4x9SM_UXjQVw",
        "outputId": "08b7a8f7-fbab-41f9-9b99-f2acf57020d1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "img_train = img_train.astype('float32') / 255.0\n",
        "img_test = img_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "SESm8EhOjMxi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yToQVUqwjVxN",
        "outputId": "184b411b-904c-4d0e-cbaf-d17319696fa9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.        , 0.        ,\n",
              "        0.05098039, 0.28627452, 0.        , 0.        , 0.00392157,\n",
              "        0.01568628, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.00392157, 0.00392157, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.01176471, 0.        , 0.14117648,\n",
              "        0.53333336, 0.49803922, 0.24313726, 0.21176471, 0.        ,\n",
              "        0.        , 0.        , 0.00392157, 0.01176471, 0.01568628,\n",
              "        0.        , 0.        , 0.01176471],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.02352941, 0.        , 0.4       ,\n",
              "        0.8       , 0.6901961 , 0.5254902 , 0.5647059 , 0.48235294,\n",
              "        0.09019608, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.04705882, 0.03921569, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.60784316,\n",
              "        0.9254902 , 0.8117647 , 0.69803923, 0.41960785, 0.6117647 ,\n",
              "        0.6313726 , 0.42745098, 0.2509804 , 0.09019608, 0.3019608 ,\n",
              "        0.50980395, 0.28235295, 0.05882353],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.00392157, 0.        , 0.27058825, 0.8117647 ,\n",
              "        0.8745098 , 0.85490197, 0.84705883, 0.84705883, 0.6392157 ,\n",
              "        0.49803922, 0.4745098 , 0.47843137, 0.57254905, 0.5529412 ,\n",
              "        0.34509805, 0.6745098 , 0.25882354],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.00392157, 0.00392157, 0.        , 0.78431374, 0.9098039 ,\n",
              "        0.9098039 , 0.9137255 , 0.8980392 , 0.8745098 , 0.8745098 ,\n",
              "        0.84313726, 0.8352941 , 0.6431373 , 0.49803922, 0.48235294,\n",
              "        0.76862746, 0.8980392 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.7176471 , 0.88235295,\n",
              "        0.84705883, 0.8745098 , 0.89411765, 0.92156863, 0.8901961 ,\n",
              "        0.8784314 , 0.87058824, 0.8784314 , 0.8666667 , 0.8745098 ,\n",
              "        0.9607843 , 0.6784314 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.75686276, 0.89411765,\n",
              "        0.85490197, 0.8352941 , 0.7764706 , 0.7058824 , 0.83137256,\n",
              "        0.8235294 , 0.827451  , 0.8352941 , 0.8745098 , 0.8627451 ,\n",
              "        0.9529412 , 0.7921569 , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.01176471, 0.        , 0.04705882, 0.85882354, 0.8627451 ,\n",
              "        0.83137256, 0.85490197, 0.7529412 , 0.6627451 , 0.8901961 ,\n",
              "        0.8156863 , 0.85490197, 0.8784314 , 0.83137256, 0.8862745 ,\n",
              "        0.77254903, 0.81960785, 0.20392157],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.02352941, 0.        , 0.3882353 , 0.95686275, 0.87058824,\n",
              "        0.8627451 , 0.85490197, 0.79607844, 0.7764706 , 0.8666667 ,\n",
              "        0.84313726, 0.8352941 , 0.87058824, 0.8627451 , 0.9607843 ,\n",
              "        0.46666667, 0.654902  , 0.21960784],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.01568628,\n",
              "        0.        , 0.        , 0.21568628, 0.9254902 , 0.89411765,\n",
              "        0.9019608 , 0.89411765, 0.9411765 , 0.9098039 , 0.8352941 ,\n",
              "        0.85490197, 0.8745098 , 0.91764706, 0.8509804 , 0.8509804 ,\n",
              "        0.81960785, 0.36078432, 0.        ],\n",
              "       [0.        , 0.        , 0.00392157, 0.01568628, 0.02352941,\n",
              "        0.02745098, 0.00784314, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.92941177, 0.8862745 , 0.8509804 ,\n",
              "        0.8745098 , 0.87058824, 0.85882354, 0.87058824, 0.8666667 ,\n",
              "        0.84705883, 0.8745098 , 0.8980392 , 0.84313726, 0.85490197,\n",
              "        1.        , 0.3019608 , 0.        ],\n",
              "       [0.        , 0.01176471, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.24313726,\n",
              "        0.5686275 , 0.8       , 0.89411765, 0.8117647 , 0.8352941 ,\n",
              "        0.8666667 , 0.85490197, 0.8156863 , 0.827451  , 0.85490197,\n",
              "        0.8784314 , 0.8745098 , 0.85882354, 0.84313726, 0.8784314 ,\n",
              "        0.95686275, 0.62352943, 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.07058824,\n",
              "        0.17254902, 0.32156864, 0.41960785, 0.7411765 , 0.89411765,\n",
              "        0.8627451 , 0.87058824, 0.8509804 , 0.8862745 , 0.78431374,\n",
              "        0.8039216 , 0.827451  , 0.9019608 , 0.8784314 , 0.91764706,\n",
              "        0.6901961 , 0.7372549 , 0.98039216, 0.972549  , 0.9137255 ,\n",
              "        0.93333334, 0.84313726, 0.        ],\n",
              "       [0.        , 0.22352941, 0.73333335, 0.8156863 , 0.8784314 ,\n",
              "        0.8666667 , 0.8784314 , 0.8156863 , 0.8       , 0.8392157 ,\n",
              "        0.8156863 , 0.81960785, 0.78431374, 0.62352943, 0.9607843 ,\n",
              "        0.75686276, 0.80784315, 0.8745098 , 1.        , 1.        ,\n",
              "        0.8666667 , 0.91764706, 0.8666667 , 0.827451  , 0.8627451 ,\n",
              "        0.9098039 , 0.9647059 , 0.        ],\n",
              "       [0.01176471, 0.7921569 , 0.89411765, 0.8784314 , 0.8666667 ,\n",
              "        0.827451  , 0.827451  , 0.8392157 , 0.8039216 , 0.8039216 ,\n",
              "        0.8039216 , 0.8627451 , 0.9411765 , 0.3137255 , 0.5882353 ,\n",
              "        1.        , 0.8980392 , 0.8666667 , 0.7372549 , 0.6039216 ,\n",
              "        0.7490196 , 0.8235294 , 0.8       , 0.81960785, 0.87058824,\n",
              "        0.89411765, 0.88235295, 0.        ],\n",
              "       [0.38431373, 0.9137255 , 0.7764706 , 0.8235294 , 0.87058824,\n",
              "        0.8980392 , 0.8980392 , 0.91764706, 0.9764706 , 0.8627451 ,\n",
              "        0.7607843 , 0.84313726, 0.8509804 , 0.94509804, 0.25490198,\n",
              "        0.28627452, 0.41568628, 0.45882353, 0.65882355, 0.85882354,\n",
              "        0.8666667 , 0.84313726, 0.8509804 , 0.8745098 , 0.8745098 ,\n",
              "        0.8784314 , 0.8980392 , 0.11372549],\n",
              "       [0.29411766, 0.8       , 0.83137256, 0.8       , 0.75686276,\n",
              "        0.8039216 , 0.827451  , 0.88235295, 0.84705883, 0.7254902 ,\n",
              "        0.77254903, 0.80784315, 0.7764706 , 0.8352941 , 0.9411765 ,\n",
              "        0.7647059 , 0.8901961 , 0.9607843 , 0.9372549 , 0.8745098 ,\n",
              "        0.85490197, 0.83137256, 0.81960785, 0.87058824, 0.8627451 ,\n",
              "        0.8666667 , 0.9019608 , 0.2627451 ],\n",
              "       [0.1882353 , 0.79607844, 0.7176471 , 0.7607843 , 0.8352941 ,\n",
              "        0.77254903, 0.7254902 , 0.74509805, 0.7607843 , 0.7529412 ,\n",
              "        0.7921569 , 0.8392157 , 0.85882354, 0.8666667 , 0.8627451 ,\n",
              "        0.9254902 , 0.88235295, 0.84705883, 0.78039217, 0.80784315,\n",
              "        0.7294118 , 0.70980394, 0.69411767, 0.6745098 , 0.70980394,\n",
              "        0.8039216 , 0.80784315, 0.4509804 ],\n",
              "       [0.        , 0.47843137, 0.85882354, 0.75686276, 0.7019608 ,\n",
              "        0.67058825, 0.7176471 , 0.76862746, 0.8       , 0.8235294 ,\n",
              "        0.8352941 , 0.8117647 , 0.827451  , 0.8235294 , 0.78431374,\n",
              "        0.76862746, 0.7607843 , 0.7490196 , 0.7647059 , 0.7490196 ,\n",
              "        0.7764706 , 0.7529412 , 0.6901961 , 0.6117647 , 0.654902  ,\n",
              "        0.69411767, 0.8235294 , 0.36078432],\n",
              "       [0.        , 0.        , 0.2901961 , 0.7411765 , 0.83137256,\n",
              "        0.7490196 , 0.6862745 , 0.6745098 , 0.6862745 , 0.70980394,\n",
              "        0.7254902 , 0.7372549 , 0.7411765 , 0.7372549 , 0.75686276,\n",
              "        0.7764706 , 0.8       , 0.81960785, 0.8235294 , 0.8235294 ,\n",
              "        0.827451  , 0.7372549 , 0.7372549 , 0.7607843 , 0.7529412 ,\n",
              "        0.84705883, 0.6666667 , 0.        ],\n",
              "       [0.00784314, 0.        , 0.        , 0.        , 0.25882354,\n",
              "        0.78431374, 0.87058824, 0.92941177, 0.9372549 , 0.9490196 ,\n",
              "        0.9647059 , 0.9529412 , 0.95686275, 0.8666667 , 0.8627451 ,\n",
              "        0.75686276, 0.7490196 , 0.7019608 , 0.7137255 , 0.7137255 ,\n",
              "        0.70980394, 0.6901961 , 0.6509804 , 0.65882355, 0.3882353 ,\n",
              "        0.22745098, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.15686275, 0.23921569, 0.17254902,\n",
              "        0.28235295, 0.16078432, 0.13725491, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a hypermodel.**"
      ],
      "metadata": {
        "id": "Dg5kytSCjiqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_builder(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "  # Tune the number of units in the first Dense layer\n",
        "  # Choose an optimal value between 32-512\n",
        "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
        "  model.add(keras.layers.Dense(10))\n",
        "\n",
        "  # Tune the learning rate for the optimizer\n",
        "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "ObqO_3Kpjr0L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - \n",
        "RandomSearch,\n",
        "Hyperband, \n",
        "BayesianOptimization, and \n",
        "Sklearn. \n",
        "In this tutorial, you use the Hyperband tuner.**"
      ],
      "metadata": {
        "id": "pL9QyHi5kBR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KY8vZhqlkKRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(model_builder,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=5,\n",
        "                     factor=3,\n",
        "                     directory='my_dir',\n",
        "                     project_name='intro_to_kt')"
      ],
      "metadata": {
        "id": "m_ZLld4ZkDan"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a callback to stop training early after reaching a certain value for the validation loss.**"
      ],
      "metadata": {
        "id": "-mBxmlaZkVht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "metadata": {
        "id": "OYHwL4IvkTrz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(img_train, label_train, epochs=5, validation_split=0.2, callbacks=[stop_early])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
        "is {best_hps.get('learning_rate')}.\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_FkUtRbWkt1N",
        "outputId": "ed7715b6-30ea-4d21-e0b5-7c0a113c4d96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
            "layer is 288 and the optimal learning rate for the optimizer\n",
            "is 0.001.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find the optimal number of epochs to train the model with the hyperparameters obtained from the search.**"
      ],
      "metadata": {
        "id": "9PaQHTHKm83T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
        "\n",
        "val_acc_per_epoch = history.history['val_accuracy']\n",
        "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
        "print('Best epoch: %d' % (best_epoch,))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SBDvkzXcnAgM",
        "outputId": "5f00fe5a-cfc3-45fc-daf7-725f9b7d2cd6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4982 - accuracy: 0.8230 - val_loss: 0.4118 - val_accuracy: 0.8541\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3714 - accuracy: 0.8643 - val_loss: 0.3737 - val_accuracy: 0.8668\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3304 - accuracy: 0.8789 - val_loss: 0.4085 - val_accuracy: 0.8505\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3092 - accuracy: 0.8856 - val_loss: 0.3324 - val_accuracy: 0.8778\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2874 - accuracy: 0.8928 - val_loss: 0.3350 - val_accuracy: 0.8773\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2729 - accuracy: 0.8985 - val_loss: 0.3275 - val_accuracy: 0.8808\n",
            "Epoch 7/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2601 - accuracy: 0.9034 - val_loss: 0.3281 - val_accuracy: 0.8842\n",
            "Epoch 8/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2507 - accuracy: 0.9065 - val_loss: 0.3633 - val_accuracy: 0.8729\n",
            "Epoch 9/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2382 - accuracy: 0.9125 - val_loss: 0.3395 - val_accuracy: 0.8798\n",
            "Epoch 10/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2276 - accuracy: 0.9146 - val_loss: 0.3137 - val_accuracy: 0.8902\n",
            "Epoch 11/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2218 - accuracy: 0.9179 - val_loss: 0.3134 - val_accuracy: 0.8889\n",
            "Epoch 12/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2102 - accuracy: 0.9208 - val_loss: 0.3162 - val_accuracy: 0.8906\n",
            "Epoch 13/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2046 - accuracy: 0.9227 - val_loss: 0.3350 - val_accuracy: 0.8823\n",
            "Epoch 14/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1957 - accuracy: 0.9266 - val_loss: 0.3147 - val_accuracy: 0.8898\n",
            "Epoch 15/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1886 - accuracy: 0.9295 - val_loss: 0.3561 - val_accuracy: 0.8827\n",
            "Epoch 16/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1835 - accuracy: 0.9325 - val_loss: 0.3314 - val_accuracy: 0.8874\n",
            "Epoch 17/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1753 - accuracy: 0.9337 - val_loss: 0.3414 - val_accuracy: 0.8903\n",
            "Epoch 18/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1720 - accuracy: 0.9353 - val_loss: 0.3362 - val_accuracy: 0.8907\n",
            "Epoch 19/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1640 - accuracy: 0.9380 - val_loss: 0.3382 - val_accuracy: 0.8914\n",
            "Epoch 20/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1594 - accuracy: 0.9398 - val_loss: 0.3413 - val_accuracy: 0.8940\n",
            "Epoch 21/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1566 - accuracy: 0.9415 - val_loss: 0.3419 - val_accuracy: 0.8942\n",
            "Epoch 22/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1500 - accuracy: 0.9430 - val_loss: 0.3418 - val_accuracy: 0.8907\n",
            "Epoch 23/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1458 - accuracy: 0.9445 - val_loss: 0.3534 - val_accuracy: 0.8931\n",
            "Epoch 24/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1430 - accuracy: 0.9457 - val_loss: 0.3734 - val_accuracy: 0.8925\n",
            "Epoch 25/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1385 - accuracy: 0.9474 - val_loss: 0.3770 - val_accuracy: 0.8904\n",
            "Epoch 26/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1355 - accuracy: 0.9491 - val_loss: 0.3753 - val_accuracy: 0.8906\n",
            "Epoch 27/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1328 - accuracy: 0.9502 - val_loss: 0.3837 - val_accuracy: 0.8912\n",
            "Epoch 28/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1299 - accuracy: 0.9510 - val_loss: 0.3954 - val_accuracy: 0.8898\n",
            "Epoch 29/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1243 - accuracy: 0.9534 - val_loss: 0.4111 - val_accuracy: 0.8942\n",
            "Epoch 30/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1219 - accuracy: 0.9545 - val_loss: 0.3996 - val_accuracy: 0.8881\n",
            "Epoch 31/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1164 - accuracy: 0.9563 - val_loss: 0.4040 - val_accuracy: 0.8940\n",
            "Epoch 32/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1165 - accuracy: 0.9564 - val_loss: 0.4151 - val_accuracy: 0.8903\n",
            "Epoch 33/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1130 - accuracy: 0.9571 - val_loss: 0.4107 - val_accuracy: 0.8928\n",
            "Epoch 34/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1098 - accuracy: 0.9584 - val_loss: 0.4185 - val_accuracy: 0.8916\n",
            "Epoch 35/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1086 - accuracy: 0.9585 - val_loss: 0.4401 - val_accuracy: 0.8919\n",
            "Epoch 36/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1048 - accuracy: 0.9595 - val_loss: 0.4330 - val_accuracy: 0.8910\n",
            "Epoch 37/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1000 - accuracy: 0.9619 - val_loss: 0.4483 - val_accuracy: 0.8903\n",
            "Epoch 38/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1019 - accuracy: 0.9616 - val_loss: 0.4790 - val_accuracy: 0.8910\n",
            "Epoch 39/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0987 - accuracy: 0.9635 - val_loss: 0.4678 - val_accuracy: 0.8913\n",
            "Epoch 40/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0938 - accuracy: 0.9645 - val_loss: 0.4676 - val_accuracy: 0.8875\n",
            "Epoch 41/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0950 - accuracy: 0.9644 - val_loss: 0.4737 - val_accuracy: 0.8915\n",
            "Epoch 42/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0898 - accuracy: 0.9668 - val_loss: 0.4847 - val_accuracy: 0.8921\n",
            "Epoch 43/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0912 - accuracy: 0.9658 - val_loss: 0.4842 - val_accuracy: 0.8875\n",
            "Epoch 44/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0906 - accuracy: 0.9660 - val_loss: 0.5283 - val_accuracy: 0.8880\n",
            "Epoch 45/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0864 - accuracy: 0.9674 - val_loss: 0.5279 - val_accuracy: 0.8903\n",
            "Epoch 46/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0807 - accuracy: 0.9697 - val_loss: 0.5065 - val_accuracy: 0.8929\n",
            "Epoch 47/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0828 - accuracy: 0.9684 - val_loss: 0.5200 - val_accuracy: 0.8909\n",
            "Epoch 48/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0810 - accuracy: 0.9697 - val_loss: 0.5253 - val_accuracy: 0.8898\n",
            "Epoch 49/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0801 - accuracy: 0.9695 - val_loss: 0.5178 - val_accuracy: 0.8896\n",
            "Epoch 50/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0774 - accuracy: 0.9707 - val_loss: 0.5170 - val_accuracy: 0.8950\n",
            "Best epoch: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-instantiate the hypermodel and train it with the optimal number of epochs from above"
      ],
      "metadata": {
        "id": "brkCpC_tnHlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Retrain the model\n",
        "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sZudXwn_nIhU",
        "outputId": "4f82c45e-82c3-4e5d-ff6a-9f96c70ab423"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.4995 - accuracy: 0.8223 - val_loss: 0.4012 - val_accuracy: 0.8573\n",
            "Epoch 2/50\n",
            "1500/1500 [==============================] - 5s 4ms/step - loss: 0.3736 - accuracy: 0.8642 - val_loss: 0.3696 - val_accuracy: 0.8672\n",
            "Epoch 3/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3346 - accuracy: 0.8766 - val_loss: 0.3807 - val_accuracy: 0.8653\n",
            "Epoch 4/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.3070 - accuracy: 0.8861 - val_loss: 0.3530 - val_accuracy: 0.8723\n",
            "Epoch 5/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2909 - accuracy: 0.8924 - val_loss: 0.3215 - val_accuracy: 0.8833\n",
            "Epoch 6/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2735 - accuracy: 0.8978 - val_loss: 0.3149 - val_accuracy: 0.8860\n",
            "Epoch 7/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2595 - accuracy: 0.9039 - val_loss: 0.3260 - val_accuracy: 0.8813\n",
            "Epoch 8/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2457 - accuracy: 0.9084 - val_loss: 0.3022 - val_accuracy: 0.8922\n",
            "Epoch 9/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2378 - accuracy: 0.9102 - val_loss: 0.3393 - val_accuracy: 0.8842\n",
            "Epoch 10/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2282 - accuracy: 0.9141 - val_loss: 0.3062 - val_accuracy: 0.8946\n",
            "Epoch 11/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2209 - accuracy: 0.9167 - val_loss: 0.3335 - val_accuracy: 0.8851\n",
            "Epoch 12/50\n",
            "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2116 - accuracy: 0.9202 - val_loss: 0.3453 - val_accuracy: 0.8836\n",
            "Epoch 13/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.2038 - accuracy: 0.9236 - val_loss: 0.3251 - val_accuracy: 0.8903\n",
            "Epoch 14/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1953 - accuracy: 0.9261 - val_loss: 0.3206 - val_accuracy: 0.8949\n",
            "Epoch 15/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1899 - accuracy: 0.9294 - val_loss: 0.3294 - val_accuracy: 0.8919\n",
            "Epoch 16/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1842 - accuracy: 0.9307 - val_loss: 0.3384 - val_accuracy: 0.8881\n",
            "Epoch 17/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1759 - accuracy: 0.9347 - val_loss: 0.3554 - val_accuracy: 0.8913\n",
            "Epoch 18/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1713 - accuracy: 0.9354 - val_loss: 0.3345 - val_accuracy: 0.8910\n",
            "Epoch 19/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1660 - accuracy: 0.9373 - val_loss: 0.3455 - val_accuracy: 0.8898\n",
            "Epoch 20/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1620 - accuracy: 0.9386 - val_loss: 0.3366 - val_accuracy: 0.8948\n",
            "Epoch 21/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1567 - accuracy: 0.9403 - val_loss: 0.3509 - val_accuracy: 0.8930\n",
            "Epoch 22/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1520 - accuracy: 0.9419 - val_loss: 0.3604 - val_accuracy: 0.8895\n",
            "Epoch 23/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1469 - accuracy: 0.9444 - val_loss: 0.3631 - val_accuracy: 0.8909\n",
            "Epoch 24/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1418 - accuracy: 0.9461 - val_loss: 0.3908 - val_accuracy: 0.8863\n",
            "Epoch 25/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1408 - accuracy: 0.9475 - val_loss: 0.3669 - val_accuracy: 0.8922\n",
            "Epoch 26/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1350 - accuracy: 0.9501 - val_loss: 0.3972 - val_accuracy: 0.8880\n",
            "Epoch 27/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1310 - accuracy: 0.9510 - val_loss: 0.3916 - val_accuracy: 0.8907\n",
            "Epoch 28/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1268 - accuracy: 0.9524 - val_loss: 0.3903 - val_accuracy: 0.8934\n",
            "Epoch 29/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1230 - accuracy: 0.9541 - val_loss: 0.4249 - val_accuracy: 0.8867\n",
            "Epoch 30/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1235 - accuracy: 0.9543 - val_loss: 0.4184 - val_accuracy: 0.8905\n",
            "Epoch 31/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1179 - accuracy: 0.9559 - val_loss: 0.4152 - val_accuracy: 0.8913\n",
            "Epoch 32/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1177 - accuracy: 0.9553 - val_loss: 0.4650 - val_accuracy: 0.8870\n",
            "Epoch 33/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1122 - accuracy: 0.9573 - val_loss: 0.4060 - val_accuracy: 0.8936\n",
            "Epoch 34/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1089 - accuracy: 0.9594 - val_loss: 0.4117 - val_accuracy: 0.8963\n",
            "Epoch 35/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1078 - accuracy: 0.9595 - val_loss: 0.4492 - val_accuracy: 0.8932\n",
            "Epoch 36/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1070 - accuracy: 0.9599 - val_loss: 0.4185 - val_accuracy: 0.8960\n",
            "Epoch 37/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0999 - accuracy: 0.9632 - val_loss: 0.4571 - val_accuracy: 0.8934\n",
            "Epoch 38/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.1009 - accuracy: 0.9614 - val_loss: 0.4300 - val_accuracy: 0.8942\n",
            "Epoch 39/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0975 - accuracy: 0.9631 - val_loss: 0.4509 - val_accuracy: 0.8952\n",
            "Epoch 40/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0957 - accuracy: 0.9646 - val_loss: 0.4600 - val_accuracy: 0.8931\n",
            "Epoch 41/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0923 - accuracy: 0.9650 - val_loss: 0.4892 - val_accuracy: 0.8917\n",
            "Epoch 42/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0922 - accuracy: 0.9664 - val_loss: 0.4684 - val_accuracy: 0.8891\n",
            "Epoch 43/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0906 - accuracy: 0.9655 - val_loss: 0.4794 - val_accuracy: 0.8926\n",
            "Epoch 44/50\n",
            "1500/1500 [==============================] - 5s 3ms/step - loss: 0.0893 - accuracy: 0.9664 - val_loss: 0.5276 - val_accuracy: 0.8875\n",
            "Epoch 45/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0854 - accuracy: 0.9674 - val_loss: 0.5228 - val_accuracy: 0.8892\n",
            "Epoch 46/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0845 - accuracy: 0.9685 - val_loss: 0.4980 - val_accuracy: 0.8918\n",
            "Epoch 47/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0823 - accuracy: 0.9693 - val_loss: 0.5015 - val_accuracy: 0.8944\n",
            "Epoch 48/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0808 - accuracy: 0.9693 - val_loss: 0.4843 - val_accuracy: 0.8934\n",
            "Epoch 49/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0790 - accuracy: 0.9698 - val_loss: 0.5016 - val_accuracy: 0.8959\n",
            "Epoch 50/50\n",
            "1500/1500 [==============================] - 4s 3ms/step - loss: 0.0784 - accuracy: 0.9709 - val_loss: 0.4953 - val_accuracy: 0.8952\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcc8cfc2ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "  **evaluate the hypermodel on the test data**"
      ],
      "metadata": {
        "id": "xfpxwhwKnSPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result = hypermodel.evaluate(img_test, label_test)\n",
        "print(\"[test loss, test accuracy]:\", eval_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5xfm-QNAnOFh",
        "outputId": "62b45bca-f251-4fb0-a4bb-5292985a94ec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5553 - accuracy: 0.8877\n",
            "[test loss, test accuracy]: [0.5553167462348938, 0.8877000212669373]\n"
          ]
        }
      ]
    }
  ]
}